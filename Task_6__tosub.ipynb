{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1y8k-uDBPYf"
      },
      "source": [
        "## Данные\n",
        "\n",
        "Данные в [архиве](https://drive.google.com/file/d/15o7fdxTgndoy6K-e7g8g1M2-bOOwqZPl/view?usp=sharing). В нём два файла:\n",
        "- `news_train.txt` тестовое множество\n",
        "- `news_test.txt` тренировочное множество\n",
        "\n",
        "С некоторых новостных сайтов были загружены тексты новостей за период  несколько лет, причем каждая новость принаделжит к какой-то рубрике: `science`, `style`, `culture`, `life`, `economics`, `business`, `travel`, `forces`, `media`, `sport`.\n",
        "\n",
        "В каждой строке файла содержится метка рубрики, заголовок новостной статьи и сам текст статьи, например:\n",
        "\n",
        ">    **sport**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею разгромила чехов**&nbsp;&lt;tab&gt;&nbsp;**Сборная Канады по хоккею крупно об...**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd C:\\Users\\Michel\\DataspellProjects\\dsProject\\docs_classif\\task6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGbBJWBPBRkC",
        "outputId": "c994b5d2-2bea-421c-d468-203598cf7db0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C:\\Users\\Michel\\DataspellProjects\\dsProject\\docs_classif\\task6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K72fZKa4BPYi"
      },
      "source": [
        "# Задача\n",
        "\n",
        "1. Обработать данные, получив для каждого текста набор токенов\n",
        "Обработать токены с помощью (один вариант из трех):\n",
        "    - pymorphy2\n",
        "    - русского [snowball стеммера](https://www.nltk.org/howto/stem.html)\n",
        "    - [SentencePiece](https://github.com/google/sentencepiece) или [Huggingface Tokenizers](https://github.com/huggingface/tokenizers)\n",
        "    \n",
        "    \n",
        "2. Обучить word embeddings (fastText, word2vec, gloVe) на тренировочных данных. Можно использовать [gensim](https://radimrehurek.com/gensim/models/word2vec.html) . Продемонстрировать семантические ассоциации. \n",
        "\n",
        "3. Реализовать алгоритм классификации документа по категориям, посчитать точноть на тестовых данных, подобрать гиперпараметры. Метод векторизации выбрать произвольно - можно использовать $tf-idf$ с понижением размерности (см. scikit-learn), можно использовать обученные на предыдущем шаге векторные представления, можно использовать [предобученные модели](https://rusvectores.org/ru/models/). Имейте ввиду, что простое \"усреднение\" токенов в тексте скорее всего не даст положительных результатов. Нужно реализовать два алгоритмов из трех:\n",
        "     - SVM\n",
        "     - наивный байесовский классификатор\n",
        "     - логистическая регрессия\n",
        "    \n",
        "\n",
        "4.* Реализуйте классификацию с помощью нейросетевых моделей. Например [RuBERT](http://docs.deeppavlov.ai/en/master/features/models/bert.html) или [ELMo](https://rusvectores.org/ru/models/)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task1"
      ],
      "metadata": {
        "id": "p7cIKdCOBkpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "import string"
      ],
      "metadata": {
        "id": "I50kr2u3nHRV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "nltk.download('stopwords')\n",
        "stop_words=nltk.corpus.stopwords.words('russian')\n",
        "stop_words"
      ],
      "metadata": {
        "id": "7oz1uC9YsutS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сделаем датасет, предварительно убрав стоп слова и пунктуацию. Приведем слова к нормальной форме с помощью pymorphy2"
      ],
      "metadata": {
        "id": "riCRkNN1pfDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.DataFrame({'theme':[],'text':[],'text_normalized':[]})\n",
        "idx = 0\n",
        "vocalulary_train = []\n",
        "with open('news_train.txt', encoding='utf8') as f_train:\n",
        "  for line in f_train:\n",
        "    line = re.sub(r'[^\\w\\s]','',line)\n",
        "    line = re.split(r'\\t', line)\n",
        "    text = ' '.join(line[1:])\n",
        "    normal_line =''\n",
        "    for word in text.split():\n",
        "      if word in stop_words:\n",
        "        continue\n",
        "      normal_word = morph.parse(word)[0].normal_form\n",
        "      vocalulary_train.append(normal_word)\n",
        "      normal_line=' '.join((normal_line,normal_word))\n",
        "    df_train.loc[idx] = [line[0],text,normal_line]\n",
        "    idx+=1"
      ],
      "metadata": {
        "id": "XvK9G226Boda"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.DataFrame({'theme':[],'text':[],'text_normalized':[]})\n",
        "idx = 0\n",
        "vocalulary_test = []\n",
        "with open('news_test.txt', encoding='utf8') as f_test:\n",
        "  for line in f_test:\n",
        "    line = re.sub(r'[^\\w\\s]','',line)\n",
        "    line = re.split(r'\\t', line)\n",
        "    text = ' '.join(line[1:])\n",
        "    normal_line =''\n",
        "    for word in text.split():\n",
        "      if word in stop_words:\n",
        "        continue\n",
        "      normal_word = morph.parse(word)[0].normal_form\n",
        "      vocalulary_test.append(normal_word)\n",
        "      normal_line=' '.join((normal_line,normal_word))\n",
        "    df_test.loc[idx] = [line[0],text,normal_line]\n",
        "    idx+=1"
      ],
      "metadata": {
        "id": "xTUibv30safJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "lXfJiQ-3_ZI_",
        "outputId": "5d286b2d-3322-4807-8c69-567ad6aa0f01"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     theme                                               text  \\\n",
              "0    sport  Овечкин пожертвовал детской хоккейной школе ав...   \n",
              "1  culture  Рекордно дорогую статую майя признали подделко...   \n",
              "2  science  Samsung представила флагман в защищенном корпу...   \n",
              "3    sport  С футболиста Спартака сняли четырехматчевую ди...   \n",
              "4    media  Hopes  Fears объединится с The Village Интерне...   \n",
              "\n",
              "                                     text_normalized  \n",
              "0   овечкин пожертвовать детский хоккейный школа ...  \n",
              "1   рекордно дорогой статуя майя признать подделк...  \n",
              "2   samsung представить флагман защитить корпус ю...  \n",
              "3   с футболист спартак снять четырехматчевой дис...  \n",
              "4   hopes fears объединиться the village интернет...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>theme</th>\n",
              "      <th>text</th>\n",
              "      <th>text_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sport</td>\n",
              "      <td>Овечкин пожертвовал детской хоккейной школе ав...</td>\n",
              "      <td>овечкин пожертвовать детский хоккейный школа ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>culture</td>\n",
              "      <td>Рекордно дорогую статую майя признали подделко...</td>\n",
              "      <td>рекордно дорогой статуя майя признать подделк...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>science</td>\n",
              "      <td>Samsung представила флагман в защищенном корпу...</td>\n",
              "      <td>samsung представить флагман защитить корпус ю...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sport</td>\n",
              "      <td>С футболиста Спартака сняли четырехматчевую ди...</td>\n",
              "      <td>с футболист спартак снять четырехматчевой дис...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>media</td>\n",
              "      <td>Hopes  Fears объединится с The Village Интерне...</td>\n",
              "      <td>hopes fears объединиться the village интернет...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "hA7Ssi80s379",
        "outputId": "41bf8c8f-b50f-4e49-9961-f96eadc9a227"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      theme                                               text  \\\n",
              "0   culture  Жительница Ямала победила в первом песенном ко...   \n",
              "1     media  Почти половина Twitterпользователей никогда не...   \n",
              "2     media  Билайн начал рекламу роуминга под песенку Трол...   \n",
              "3  business  Saipem потеряла 12 миллиарда евро изза отмены ...   \n",
              "4   culture  Вин Дизель назвал Форсаж 7 достойным Оскара Ак...   \n",
              "\n",
              "                                     text_normalized  \n",
              "0   жительница ямал победить первый песенный конк...  \n",
              "1   почти половина twitterпользователь писать соо...  \n",
              "2   билайн начать реклама роуминг песенка трололо...  \n",
              "3   saipem потерять 12 миллиард евро изз отмена ю...  \n",
              "4   вино дизель назвать форсаж 7 достойный оскар ...  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>theme</th>\n",
              "      <th>text</th>\n",
              "      <th>text_normalized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>culture</td>\n",
              "      <td>Жительница Ямала победила в первом песенном ко...</td>\n",
              "      <td>жительница ямал победить первый песенный конк...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>media</td>\n",
              "      <td>Почти половина Twitterпользователей никогда не...</td>\n",
              "      <td>почти половина twitterпользователь писать соо...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>media</td>\n",
              "      <td>Билайн начал рекламу роуминга под песенку Трол...</td>\n",
              "      <td>билайн начать реклама роуминг песенка трололо...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>business</td>\n",
              "      <td>Saipem потеряла 12 миллиарда евро изза отмены ...</td>\n",
              "      <td>saipem потерять 12 миллиард евро изз отмена ю...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>culture</td>\n",
              "      <td>Вин Дизель назвал Форсаж 7 достойным Оскара Ак...</td>\n",
              "      <td>вино дизель назвать форсаж 7 достойный оскар ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary_train = set(vocalulary_train)\n",
        "vocabulary_test = set(vocalulary_test)"
      ],
      "metadata": {
        "id": "mrkQfJwxs8Sx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task2"
      ],
      "metadata": {
        "id": "in9NnBQzzK6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_train = '  '.join(df_train['text_normalized'])\n",
        "text_test = '  '.join(df_test['text_normalized'])"
      ],
      "metadata": {
        "id": "CLUEYtacy6Qo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('normalized_train.txt','w', encoding='utf8') as f_train:\n",
        "#   f_train.write(text_train)"
      ],
      "metadata": {
        "id": "y3po11D6Xaw9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('normalized_test.txt','w', encoding='utf8') as f_test:\n",
        "  f_test.write(text_test)"
      ],
      "metadata": {
        "id": "kZBNarhU0aLX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получим эмбеддинги слов с помощью fasttext"
      ],
      "metadata": {
        "id": "z_cvtFubp_0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fasttext\n",
        "model = fasttext.train_unsupervised('normalized_train.txt', model='cbow')"
      ],
      "metadata": {
        "id": "2zJo5MIy3KaI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(model.words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sq-8yuY88sfm",
        "outputId": "fdbcac8d-cb5f-40e1-b983-69f9ee0d0377"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "27925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем 10 ближайших слов к словам 'кошка' и 'собака', тем самым продемострируем семантические ассоциации "
      ],
      "metadata": {
        "id": "gkLw2AjnqVO2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors('кошка')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_ZmZPpZBn6H",
        "outputId": "7d11290d-1857-43ea-b89c-5a9c7c61b2ad"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9339474439620972, 'галушка'),\n",
              " (0.9324925541877747, 'мушка'),\n",
              " (0.9273046255111694, 'ловушка'),\n",
              " (0.9190776348114014, 'кукушка'),\n",
              " (0.9167001843452454, 'лягушка'),\n",
              " (0.9102931022644043, 'бабушка'),\n",
              " (0.9070371389389038, 'шкатулка'),\n",
              " (0.899564802646637, 'собака'),\n",
              " (0.8991677761077881, 'кормушка'),\n",
              " (0.8927677869796753, 'чашка')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.get_nearest_neighbors('собака')"
      ],
      "metadata": {
        "id": "8V7c32V0qSEU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6a0564-c8ec-44bb-9aaf-e5e82590e255"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.8995648622512817, 'кошка'),\n",
              " (0.8853743076324463, 'котёнок'),\n",
              " (0.8842496275901794, 'козлёнок'),\n",
              " (0.876847505569458, 'лосёнок'),\n",
              " (0.8726280331611633, 'собачка'),\n",
              " (0.8709497451782227, 'кличка'),\n",
              " (0.8495503067970276, 'тигрёнок'),\n",
              " (0.8482656478881836, 'львёнок'),\n",
              " (0.8463788032531738, 'ягнёнок'),\n",
              " (0.8460367321968079, 'кот')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 3"
      ],
      "metadata": {
        "id": "v-psM9osNV4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим SVM и лог регрессию. Получим эмбеддинг предложения за счет усреднения эмбеддингов слов в предложении"
      ],
      "metadata": {
        "id": "S_BKNQwvNr5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_theme = list(set(df_train['theme']))\n",
        "theme_idx = dict()\n",
        "for i in range(len(list_theme)):\n",
        "  theme_idx[list_theme[i]]=i\n",
        "list_theme_train = []\n",
        "for theme in df_train['theme']:\n",
        "  list_theme_train.append(theme_idx[theme])\n",
        "list_theme_test = []\n",
        "for theme in df_test['theme']:\n",
        "  list_theme_test.append(theme_idx[theme])"
      ],
      "metadata": {
        "id": "OlkAutVbVMLP"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "targets = []\n",
        "for line in df_train['text_normalized']:\n",
        "  seq_emb = np.zeros((1,100))\n",
        "  count=0\n",
        "  for word in line.split():\n",
        "    seq_emb+=np.array(model[word])\n",
        "    count+=1\n",
        "  seq_emb=seq_emb/count\n",
        "  df_list.append(seq_emb)\n",
        "len(df_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Veay-S6gCKgw",
        "outputId": "8a7570a1-dd46-4173-c78d-5b9e5a04cd79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_train=pd.DataFrame(data=np.squeeze(np.array(df_list)))\n",
        "df_data_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "IOE5ulsQQKmv",
        "outputId": "9145b170-1cff-4e80-fc61-1c3366cca4df"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0 -0.846750 -0.211703 -0.343574 -0.318169 -0.313551  0.409213 -0.077015   \n",
              "1 -0.406527 -0.086954  0.108069 -0.218579  0.312164  0.449005  0.192080   \n",
              "2 -0.128463  0.081608 -0.509347 -0.072902  0.422272  0.075654 -0.072248   \n",
              "3 -1.397803  0.104870 -0.291781 -0.399679  0.207858  0.393510  0.441083   \n",
              "4 -0.319571 -0.744991 -0.573416 -0.018779  0.608332  0.609178  0.135325   \n",
              "\n",
              "         7         8         9   ...        90        91        92        93  \\\n",
              "0 -0.783134 -0.372726 -0.102156  ...  0.328988 -0.135662 -0.149695 -0.123620   \n",
              "1 -0.067015 -0.272677  0.317649  ... -0.379923  0.064984  0.250864  0.047013   \n",
              "2  0.200054 -0.548765  0.057418  ... -0.578394  0.372366 -0.138986  0.549562   \n",
              "3 -0.622452 -0.094257 -0.501257  ...  0.071181  0.035838 -0.558536 -0.480131   \n",
              "4 -0.035312 -0.738355  0.354851  ...  0.316036  0.339726  0.269105  0.469213   \n",
              "\n",
              "         94        95        96        97        98        99  \n",
              "0  0.186095  0.118507  0.303843  0.080714  0.160952 -0.156663  \n",
              "1  0.235075 -0.583004  0.033285  0.477899  0.325165 -0.202244  \n",
              "2  0.319726 -0.272478 -0.323735 -0.271193 -0.066781 -0.230923  \n",
              "3  0.387728  0.218733  0.046231 -0.189890  0.175672 -0.237542  \n",
              "4  0.216651  0.085785  0.021640  0.331390  0.439917 -0.481991  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.846750</td>\n",
              "      <td>-0.211703</td>\n",
              "      <td>-0.343574</td>\n",
              "      <td>-0.318169</td>\n",
              "      <td>-0.313551</td>\n",
              "      <td>0.409213</td>\n",
              "      <td>-0.077015</td>\n",
              "      <td>-0.783134</td>\n",
              "      <td>-0.372726</td>\n",
              "      <td>-0.102156</td>\n",
              "      <td>...</td>\n",
              "      <td>0.328988</td>\n",
              "      <td>-0.135662</td>\n",
              "      <td>-0.149695</td>\n",
              "      <td>-0.123620</td>\n",
              "      <td>0.186095</td>\n",
              "      <td>0.118507</td>\n",
              "      <td>0.303843</td>\n",
              "      <td>0.080714</td>\n",
              "      <td>0.160952</td>\n",
              "      <td>-0.156663</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.406527</td>\n",
              "      <td>-0.086954</td>\n",
              "      <td>0.108069</td>\n",
              "      <td>-0.218579</td>\n",
              "      <td>0.312164</td>\n",
              "      <td>0.449005</td>\n",
              "      <td>0.192080</td>\n",
              "      <td>-0.067015</td>\n",
              "      <td>-0.272677</td>\n",
              "      <td>0.317649</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.379923</td>\n",
              "      <td>0.064984</td>\n",
              "      <td>0.250864</td>\n",
              "      <td>0.047013</td>\n",
              "      <td>0.235075</td>\n",
              "      <td>-0.583004</td>\n",
              "      <td>0.033285</td>\n",
              "      <td>0.477899</td>\n",
              "      <td>0.325165</td>\n",
              "      <td>-0.202244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.128463</td>\n",
              "      <td>0.081608</td>\n",
              "      <td>-0.509347</td>\n",
              "      <td>-0.072902</td>\n",
              "      <td>0.422272</td>\n",
              "      <td>0.075654</td>\n",
              "      <td>-0.072248</td>\n",
              "      <td>0.200054</td>\n",
              "      <td>-0.548765</td>\n",
              "      <td>0.057418</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.578394</td>\n",
              "      <td>0.372366</td>\n",
              "      <td>-0.138986</td>\n",
              "      <td>0.549562</td>\n",
              "      <td>0.319726</td>\n",
              "      <td>-0.272478</td>\n",
              "      <td>-0.323735</td>\n",
              "      <td>-0.271193</td>\n",
              "      <td>-0.066781</td>\n",
              "      <td>-0.230923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.397803</td>\n",
              "      <td>0.104870</td>\n",
              "      <td>-0.291781</td>\n",
              "      <td>-0.399679</td>\n",
              "      <td>0.207858</td>\n",
              "      <td>0.393510</td>\n",
              "      <td>0.441083</td>\n",
              "      <td>-0.622452</td>\n",
              "      <td>-0.094257</td>\n",
              "      <td>-0.501257</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071181</td>\n",
              "      <td>0.035838</td>\n",
              "      <td>-0.558536</td>\n",
              "      <td>-0.480131</td>\n",
              "      <td>0.387728</td>\n",
              "      <td>0.218733</td>\n",
              "      <td>0.046231</td>\n",
              "      <td>-0.189890</td>\n",
              "      <td>0.175672</td>\n",
              "      <td>-0.237542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.319571</td>\n",
              "      <td>-0.744991</td>\n",
              "      <td>-0.573416</td>\n",
              "      <td>-0.018779</td>\n",
              "      <td>0.608332</td>\n",
              "      <td>0.609178</td>\n",
              "      <td>0.135325</td>\n",
              "      <td>-0.035312</td>\n",
              "      <td>-0.738355</td>\n",
              "      <td>0.354851</td>\n",
              "      <td>...</td>\n",
              "      <td>0.316036</td>\n",
              "      <td>0.339726</td>\n",
              "      <td>0.269105</td>\n",
              "      <td>0.469213</td>\n",
              "      <td>0.216651</td>\n",
              "      <td>0.085785</td>\n",
              "      <td>0.021640</td>\n",
              "      <td>0.331390</td>\n",
              "      <td>0.439917</td>\n",
              "      <td>-0.481991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QszFAcAYahH9",
        "outputId": "39870e5b-dd9b-4288-c1e6-35734a77dd58"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_list = []\n",
        "targets = []\n",
        "for line in df_test['text_normalized']:\n",
        "  seq_emb = np.zeros((1,100))\n",
        "  count=0\n",
        "  for word in line.split():\n",
        "    seq_emb+=np.array(model[word])\n",
        "    count+=1\n",
        "  seq_emb=seq_emb/count\n",
        "  df_list.append(seq_emb)\n",
        "df_data_test=pd.DataFrame(data=np.squeeze(np.array(df_list)))\n",
        "df_data_test.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "fJTxD7xfYlCQ",
        "outputId": "0c0ae4e0-a514-4789-cdc1-769269ec8cbc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0 -0.717223 -0.242777 -0.168939 -0.166373 -0.071919  0.405911  0.416954   \n",
              "1 -0.813868  0.080107 -0.883001 -0.427814  0.908722  0.270789  0.321877   \n",
              "2 -0.686143 -0.367384 -0.176150 -0.234342  0.263433  0.394117  0.502982   \n",
              "3 -1.271873  0.443849 -0.104368 -0.031319  0.767693  0.548564  0.434810   \n",
              "4 -0.517246 -0.374558  0.060699  0.082716  0.282908  0.588812  0.429589   \n",
              "\n",
              "         7         8         9   ...        90        91        92        93  \\\n",
              "0 -0.196197 -0.027091 -0.234620  ... -0.048485  0.021613  0.294594 -0.193145   \n",
              "1 -0.107676 -0.816036  0.419013  ... -0.257851  0.000144  0.442114  0.012822   \n",
              "2  0.010007 -0.630374 -0.026753  ...  0.058678  0.233279  0.017320 -0.065201   \n",
              "3  0.347156 -0.010105 -0.099858  ... -0.600785  0.176777  0.322054  0.124736   \n",
              "4  0.031003 -0.246634 -0.291885  ... -0.112912  0.171904 -0.501609 -0.075209   \n",
              "\n",
              "         94        95        96        97        98        99  \n",
              "0  0.364677 -0.534842  0.304378  0.396094  0.120990  0.070339  \n",
              "1 -0.223277 -0.422229 -0.179858  0.026250  0.381324 -0.727991  \n",
              "2  0.292025 -0.241506  0.226455  0.297647  0.039798 -0.504876  \n",
              "3  0.388106 -0.410580 -0.431753  0.422078  0.227545 -0.420445  \n",
              "4  0.503934 -0.459679  0.123241  0.331275  0.075802 -0.206667  \n",
              "\n",
              "[5 rows x 100 columns]"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.717223</td>\n",
              "      <td>-0.242777</td>\n",
              "      <td>-0.168939</td>\n",
              "      <td>-0.166373</td>\n",
              "      <td>-0.071919</td>\n",
              "      <td>0.405911</td>\n",
              "      <td>0.416954</td>\n",
              "      <td>-0.196197</td>\n",
              "      <td>-0.027091</td>\n",
              "      <td>-0.234620</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048485</td>\n",
              "      <td>0.021613</td>\n",
              "      <td>0.294594</td>\n",
              "      <td>-0.193145</td>\n",
              "      <td>0.364677</td>\n",
              "      <td>-0.534842</td>\n",
              "      <td>0.304378</td>\n",
              "      <td>0.396094</td>\n",
              "      <td>0.120990</td>\n",
              "      <td>0.070339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.813868</td>\n",
              "      <td>0.080107</td>\n",
              "      <td>-0.883001</td>\n",
              "      <td>-0.427814</td>\n",
              "      <td>0.908722</td>\n",
              "      <td>0.270789</td>\n",
              "      <td>0.321877</td>\n",
              "      <td>-0.107676</td>\n",
              "      <td>-0.816036</td>\n",
              "      <td>0.419013</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.257851</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.442114</td>\n",
              "      <td>0.012822</td>\n",
              "      <td>-0.223277</td>\n",
              "      <td>-0.422229</td>\n",
              "      <td>-0.179858</td>\n",
              "      <td>0.026250</td>\n",
              "      <td>0.381324</td>\n",
              "      <td>-0.727991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.686143</td>\n",
              "      <td>-0.367384</td>\n",
              "      <td>-0.176150</td>\n",
              "      <td>-0.234342</td>\n",
              "      <td>0.263433</td>\n",
              "      <td>0.394117</td>\n",
              "      <td>0.502982</td>\n",
              "      <td>0.010007</td>\n",
              "      <td>-0.630374</td>\n",
              "      <td>-0.026753</td>\n",
              "      <td>...</td>\n",
              "      <td>0.058678</td>\n",
              "      <td>0.233279</td>\n",
              "      <td>0.017320</td>\n",
              "      <td>-0.065201</td>\n",
              "      <td>0.292025</td>\n",
              "      <td>-0.241506</td>\n",
              "      <td>0.226455</td>\n",
              "      <td>0.297647</td>\n",
              "      <td>0.039798</td>\n",
              "      <td>-0.504876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.271873</td>\n",
              "      <td>0.443849</td>\n",
              "      <td>-0.104368</td>\n",
              "      <td>-0.031319</td>\n",
              "      <td>0.767693</td>\n",
              "      <td>0.548564</td>\n",
              "      <td>0.434810</td>\n",
              "      <td>0.347156</td>\n",
              "      <td>-0.010105</td>\n",
              "      <td>-0.099858</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.600785</td>\n",
              "      <td>0.176777</td>\n",
              "      <td>0.322054</td>\n",
              "      <td>0.124736</td>\n",
              "      <td>0.388106</td>\n",
              "      <td>-0.410580</td>\n",
              "      <td>-0.431753</td>\n",
              "      <td>0.422078</td>\n",
              "      <td>0.227545</td>\n",
              "      <td>-0.420445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.517246</td>\n",
              "      <td>-0.374558</td>\n",
              "      <td>0.060699</td>\n",
              "      <td>0.082716</td>\n",
              "      <td>0.282908</td>\n",
              "      <td>0.588812</td>\n",
              "      <td>0.429589</td>\n",
              "      <td>0.031003</td>\n",
              "      <td>-0.246634</td>\n",
              "      <td>-0.291885</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.112912</td>\n",
              "      <td>0.171904</td>\n",
              "      <td>-0.501609</td>\n",
              "      <td>-0.075209</td>\n",
              "      <td>0.503934</td>\n",
              "      <td>-0.459679</td>\n",
              "      <td>0.123241</td>\n",
              "      <td>0.331275</td>\n",
              "      <td>0.075802</td>\n",
              "      <td>-0.206667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM"
      ],
      "metadata": {
        "id": "PtXbTzcogDw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "clf = make_pipeline(StandardScaler(), SVC(gamma='auto',probability=True))\n",
        "clf.fit(df_data_train,list_theme_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uw9NrUjMR0KB",
        "outputId": "990051e6-d4c4-4c29-c3f1-5a4e08f12e4f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
              "                ('svc', SVC(gamma='auto', probability=True))])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики:"
      ],
      "metadata": {
        "id": "IaTGWw57q4kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_auc_score\n",
        "pred = clf.predict(df_data_test)\n",
        "pred_proba = clf.predict_proba(df_data_test)\n",
        "print('conf_matrix:',confusion_matrix(list_theme_test,pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBt8K9blTsGY",
        "outputId": "3054cd52-e9e1-4c6c-921d-8aeb9a601e98"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conf_matrix: [[379   0   5   0   0   3   3   0  13  23]\n",
            " [  1  43   2   2   0   0   2   1   0   1]\n",
            " [  6   0 382   3   0  48   5   0  14   8]\n",
            " [  1   2   4  26   0   1  51   1   4   0]\n",
            " [  0   0   0   0 411   4   0   0   0   8]\n",
            " [  0   0   6   1   0 223   4   1   6   4]\n",
            " [  1   0   2  11   2   5 381   3  17   4]\n",
            " [  2   0   2   0   0   0   1  38   3   8]\n",
            " [  5   1  11   2   5   4  16   1 338  20]\n",
            " [ 22   3   8   0   2   3   2   1  13 361]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy:', accuracy_score(pred,list_theme_test))\n",
        "print('roc_auc:',roc_auc_score(list_theme_test,pred_proba, multi_class='ovr'))"
      ],
      "metadata": {
        "id": "ccnlu7RFrUqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df8c6591-c5d8-4ac1-8c92-5e04ea5b690c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8606666666666667\n",
            "roc_auc: 0.9855936313877265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Логистическая регрессия "
      ],
      "metadata": {
        "id": "aftar5-TgF6b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression()\n",
        "clf.fit(df_data_train,list_theme_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vukGLOPZSr3",
        "outputId": "56612b6f-089e-4935-8ed8-f8a14c448cbd"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "c:\\users\\michel\\appdata\\roaming\\jetbrains\\dataspell2021.3\\projects\\workspace\\venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метрики"
      ],
      "metadata": {
        "id": "S1r_YTRgrB2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(df_data_test)\n",
        "print('conf_matrix:',confusion_matrix(list_theme_test,pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgR4ObMqglgG",
        "outputId": "4116bc09-b6b7-4285-dfc6-aa1db39716aa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conf_matrix: [[375   0   4   0   0   5   1   1  14  26]\n",
            " [  3  41   3   2   0   0   0   1   1   1]\n",
            " [  7   0 396   1   1  32   4   0  11  14]\n",
            " [  1   0   4  31   0   3  46   0   5   0]\n",
            " [  0   0   0   0 410   3   0   0   1   9]\n",
            " [  1   0  21   1   1 204   5   2   6   4]\n",
            " [  0   1   4  21   2   5 370   2  18   3]\n",
            " [  2   0   4   1   0   1   3  32   2   9]\n",
            " [  6   1  18   1   4   4  14   1 333  21]\n",
            " [ 21   3  11   0   2   6   0   3  19 350]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_proba=clf.predict_proba(df_data_test)"
      ],
      "metadata": {
        "id": "_9m0G-FUZaU1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('accuracy:', accuracy_score(pred,list_theme_test))\n",
        "print('roc_auc:',roc_auc_score(list_theme_test,pred_proba, multi_class='ovr'))"
      ],
      "metadata": {
        "id": "OdJtjwAerS7i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad21cfb-d044-42e4-8717-fadc62b28a0c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 0.8473333333333334\n",
            "roc_auc: 0.9832361503439732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как видим, мы получили довольно хорошие скоры на тестовой выборке. SVM показал себя чуть лучше, чем лог регрессия"
      ],
      "metadata": {
        "id": "5IDANwLurEuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0ui4rScTgpqP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Task_6__tosub.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}